
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>DeepLabCut for Multi-Animal Projects &#8212; DeepLabCut</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Interactive Project Manager GUI" href="PROJECT_GUI.html" />
    <link rel="prev" title="DeepLabCut User Guide (for single animal projects)" href="standardDeepLabCut_UserGuide.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">DeepLabCut</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome!
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Main Documentation
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="README.html">
   Welcome to Our Documentation Hub!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="installation.html">
   How To Install DeepLabCut
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="standardDeepLabCut_UserGuide.html">
   DeepLabCut User Guide (for single animal projects)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   DeepLabCut for Multi-Animal Projects
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PROJECT_GUI.html">
   Interactive Project Manager GUI
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Overviewof3D.html">
   3D DeepLabCut
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="tutorial.html">
   Multi-animal pose estimation with DeepLabCut: A 5-minute tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="convert_maDLC.html">
   How to convert a pre-2.2 project for use with DeepLabCut 2.2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="HelperFunctions.html">
   Helper &amp; Advanced Optional Function Documentation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Recipes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="recipes/installTips.html">
   Installation Tips
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="recipes/io.html">
   Input/output manipulations with DeepLabCut
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="recipes/nn.html">
   Model training tips &amp; tricks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="recipes/post.html">
   Some data processing recipes!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="recipes/BatchProcessing.html">
   Automate training and video analysis: Batch Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="recipes/TechHardware.html">
   Technical (Hardware) Considerations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="recipes/DLCMethods.html">
   How to write a DLC Methods Section
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Mission &amp; Contribute
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="MISSION_AND_VALUES.html">
   Mission and Values of DeepLabCut
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="roadmap.html">
   A development roadmap for DeepLabCut
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Governance.html">
   Governance Model of DeepLabCut
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/maDLC_UserGuide.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/DeepLabCut/DeepLabCut"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/DeepLabCut/DeepLabCut/issues/new?title=Issue%20on%20page%20%2Fdocs/maDLC_UserGuide.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   DeepLabCut for Multi-Animal Projects
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-think-about-using-madlc">
     How to think about using maDLC:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#install-and-test">
     Install and test:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#get-started-in-the-terminal-or-project-gui">
     Get started in the terminal or Project GUI:
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#create-a-new-project">
       Create a New Project:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#configure-the-project">
       Configure the Project:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#select-frames-to-label">
       Select Frames to Label:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#label-frames">
       Label Frames:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#check-annotated-frames">
       Check Annotated Frames:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#create-training-dataset">
       Create Training Dataset:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#train-the-network">
       Train The Network:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#evaluate-the-trained-network">
       Evaluate the Trained Network:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decision-break-point">
       ——————– DECISION / BREAK POINT ——————-
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#attention">
         ATTENTION!
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#animal-assembly-tracking-across-frames">
       ——————- ANIMAL ASSEMBLY &amp; TRACKING ACROSS FRAMES ——————-
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#optimized-animal-assembly-video-analysis">
       Optimized Animal Assembly + Video Analysis:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#refine-tracklets">
       Refine Tracklets:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#once-you-have-analyzed-video-data-and-refined-your-madeeplabcut-tracklets">
       Once you have analyzed video data (and refined your maDeepLabCut tracklets):
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#plotting-results">
       Plotting Results:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#help">
       HELP:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tips-for-daily-use">
       Tips for “daily” use:
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#getting-help-with-madlc">
   Getting help with maDLC:
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="deeplabcut-for-multi-animal-projects">
<h1>DeepLabCut for Multi-Animal Projects<a class="headerlink" href="#deeplabcut-for-multi-animal-projects" title="Permalink to this headline">¶</a></h1>
<p>This document should serve as the user guide for maDLC,
and it is here to support the scientific advances presented in Lauer et al. 2021.</p>
<p>Note, we strongly encourage you to use the <a class="reference external" href="https://github.com/DeepLabCut/DeepLabCut/blob/master/docs/PROJECT_GUI.md">Project Manager GUI</a> when you first start using multi-animal mode. Each tab is customized for multi-animal when you create or load a multi-animal project. As long as you follow the recommendations within the GUI, you should be good to go!</p>
<div class="section" id="how-to-think-about-using-madlc">
<h2>How to think about using maDLC:<a class="headerlink" href="#how-to-think-about-using-madlc" title="Permalink to this headline">¶</a></h2>
<p>You should think of maDLC being <strong>four</strong> parts.</p>
<ul class="simple">
<li><p>(1) Curate annotation data that allows you to learn a model to track the objects/animals of interest.</p></li>
<li><p>(2) Create a high-quality pose estimation model.</p></li>
<li><p>(3) Track in space and time, i.e., assemble bodyparts to detected objects/animals and link across time. This step performs assembly and tracking (comprising first local tracking and then tracklet stitching by global reasoning).</p></li>
<li><p>(4) Any and all post-processing you wish to do with the output data, either within DLC or outside of it.</p></li>
</ul>
<p>Thus, you should always label, train, and evaluate the pose estimation performance first. If and when that performance is high, then you should go advance to the tracking step (and video analysis). There is a natural break point for this, as you will see below.</p>
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1596370260800-SP2GWKDPJCOIR7LJ31VM/ke17ZwdGBToddI8pDm48kB4fL2ovSQh5dRlH2jCMtpoUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcSV94BuD0XUinmig_1P1RJNYVU597j3jgswapL4c_w92BJE9r6UgUperYhWQ2ubQ_/workflow.png?format=2500w" width="550" title="maDLC" alt="maDLC" align="center" vspace = "50">
</div>
<div class="section" id="install-and-test">
<h2>Install and test:<a class="headerlink" href="#install-and-test" title="Permalink to this headline">¶</a></h2>
<p><strong>Quick start:</strong> If you are using DeepLabCut on the cloud, or otherwise cannot use the GUIs and you should install with: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">deeplabcut</span></code>; if you need GUI support, please use: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">'deeplabcut[gui]'</span></code>.</p>
<p>Install DLC (we recommend using our supplied conda env) then run the test script found here (you will need to git clone first):
<a class="reference external" href="https://github.com/DeepLabCut/DeepLabCut/blob/master/examples/testscript_multianimal.py">https://github.com/DeepLabCut/DeepLabCut/blob/master/examples/testscript_multianimal.py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">testscript_multianimal</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</div>
<div class="section" id="get-started-in-the-terminal-or-project-gui">
<h2>Get started in the terminal or Project GUI:<a class="headerlink" href="#get-started-in-the-terminal-or-project-gui" title="Permalink to this headline">¶</a></h2>
<p><strong>GUI:</strong> simply open your conda env, and windows/linux type <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">deeplabcut</span></code>. MacOS users: <code class="docutils literal notranslate"><span class="pre">pythonw</span> <span class="pre">-m</span> <span class="pre">deeplabcut.</span></code>
Then follow the tabs! It might be useful to read the following, however, so you understand what each command does.</p>
<p><strong>TERMINAL:</strong> To begin, (windows) navigate to anaconda prompt and right-click to “open as admin “, or (unix/MacOS) simply launch “terminal” on your computer. We assume you have DeepLabCut installed (if not, go <a class="reference external" href="https://github.com/DeepLabCut/DeepLabCut/blob/master/docs/installation.md">here</a>). Next, launch your conda env (i.e., for example <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">activate</span> <span class="pre">DLC-CPU</span></code>).</p>
<p>Start iPython, or if you are using MacOS, you must use <code class="docutils literal notranslate"><span class="pre">pythonw</span></code> vs. typing <code class="docutils literal notranslate"><span class="pre">ipython</span></code> or <code class="docutils literal notranslate"><span class="pre">python</span></code>, but otherwise it’s the same.
If you use Windows, please always open the terminal with administrator privileges. Please read more <a class="reference external" href="https://github.com/DeepLabCut/Docker4DeepLabCut2.0">here</a>, and in our Nature Protocols paper <a class="reference external" href="https://www.nature.com/articles/s41596-019-0176-0">here</a>. And, see our <a class="reference external" href="https://github.com/AlexEMG/DeepLabCut/wiki/Troubleshooting-Tips">troubleshooting wiki</a>.</p>
<p>Open an <code class="docutils literal notranslate"><span class="pre">ipython</span></code> session and import the package by typing in the terminal:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ipython</span>
<span class="kn">import</span> <span class="nn">deeplabcut</span>
</pre></div>
</div>
<p><strong>TIP:</strong> for every function there is a associated help document that can be viewed by adding a <strong>?</strong> after the function name; i.e. <code class="docutils literal notranslate"><span class="pre">deeplabcut.create_new_project?</span></code>. To exit this help screen, type <code class="docutils literal notranslate"><span class="pre">:q</span></code>.</p>
<div class="section" id="create-a-new-project">
<h3>Create a New Project:<a class="headerlink" href="#create-a-new-project" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_new_project</span><span class="p">(</span><span class="s1">&#39;ProjectName&#39;</span><span class="p">,</span><span class="s1">&#39;YourName&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;/usr/FullPath/OfVideo1.avi&#39;</span><span class="p">,</span> <span class="s1">&#39;/usr/FullPath/OfVideo2.avi&#39;</span><span class="p">,</span> <span class="s1">&#39;/usr/FullPath/OfVideo1.avi&#39;</span><span class="p">],</span>
              <span class="n">copy_videos</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">multianimal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Tip: if you want to place the project folder somewhere pass : <code class="docutils literal notranslate"><span class="pre">working_directory</span> <span class="pre">=</span> <span class="pre">'FullPathOftheworkingDirectory'</span></code></p>
<ul class="simple">
<li><p>Note, if you are a linux/macos user the path should look like: <code class="docutils literal notranslate"><span class="pre">['/home/username/yourFolder/video1.mp4']</span></code>; if you are a Windows user, it should look like: <code class="docutils literal notranslate"><span class="pre">[r'C:\username\yourFolder\video1.mp4']</span></code></p></li>
<li><p>Note, you can also put <code class="docutils literal notranslate"><span class="pre">config_path</span> <span class="pre">=</span> </code> in front of the above line to create the path to the config.yaml that is used in the next step, i.e. <code class="docutils literal notranslate"><span class="pre">config_path=deeplabcut.create_project(...)</span></code>)</p>
<ul>
<li><p>If you do not, we recommend setting a variable so this can be easily used! Once you run this step, the config_path is printed for you once you run this line, so set a variable for ease of use, i.e. something like:</p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config_path</span> <span class="o">=</span> <span class="s1">&#39;/thefulloutputpath/config.yaml&#39;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>just be mindful of the formatting for Windows vs. Unix, see above.</p></li>
</ul>
<p>This set of arguments will create a project directory with the name <strong>Name of the project+name of the experimenter+date of creation of the project</strong> in the <strong>Working directory</strong> and creates the symbolic links to videos in the <strong>videos</strong> directory. The project directory will have subdirectories: <strong>dlc-models</strong>, <strong>labeled-data</strong>, <strong>training-datasets</strong>, and <strong>videos</strong>.  All the outputs generated during the course of a project will be stored in one of these subdirectories, thus allowing each project to be curated in separation from other projects. The purpose of the subdirectories is as follows:</p>
<p><strong>dlc-models:</strong> This directory contains the subdirectories <em>test</em> and <em>train</em>, each of which holds the meta information with regard to the parameters of the feature detectors in configuration files. The configuration files are YAML files, a common human-readable data serialization language. These files can be opened and edited with standard text editors. The subdirectory <em>train</em> will store checkpoints (called snapshots in TensorFlow) during training of the model. These snapshots allow the user to reload the trained model without re-training it, or to pick-up training from a particular saved checkpoint, in case the training was interrupted.</p>
<p><strong>labeled-data:</strong> This directory will store the frames used to create the training dataset. Frames from different videos are stored in separate subdirectories. Each frame has a filename related to the temporal index within the corresponding video, which allows the user to trace every frame back to its origin.</p>
<p><strong>training-datasets:</strong>  This directory will contain the training dataset used to train the network and metadata, which contains information about how the training dataset was created.</p>
<p><strong>videos:</strong> Directory of video links or videos. When <strong>copy_videos</strong> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this directory contains symbolic links to the videos. If it is set to <code class="docutils literal notranslate"><span class="pre">True</span></code> then the videos will be copied to this directory. The default is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Additionally, if the user wants to add new videos to the project at any stage, the function <strong>add_new_videos</strong> can be used. This will update the list of videos in the project’s configuration file. Note: you neither need to use this folder for videos, nor is it required for analyzing videos (they can be anywhere).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">add_new_videos</span><span class="p">(</span><span class="s1">&#39;Full path of the project configuration file*&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;full path of video 4&#39;</span><span class="p">,</span> <span class="s1">&#39;full path of video 5&#39;</span><span class="p">],</span> <span class="n">copy_videos</span><span class="o">=</span><span class="kc">True</span><span class="o">/</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>*Please note, <em>Full path of the project configuration file</em> will be referenced as <code class="docutils literal notranslate"><span class="pre">config_path</span></code> throughout this protocol.</p>
<p>You can also use annotated data from singe-animal projects, by converting those files. There are docs for this: <a class="reference external" href="https://github.com/DeepLabCut/DeepLabCut/blob/master/docs/convert_maDLC.md">convert single to multianimal annotation data</a></p>
</div>
<div class="section" id="configure-the-project">
<h3>Configure the Project:<a class="headerlink" href="#configure-the-project" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>open the <strong>config.yaml</strong> file (in a text editor (like atom, gedit, vim etc.)), which can be found in the subfolder created when you set your project name, to change parameters and identify label names! This is a crucial step.</p></li>
</ul>
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1588892210304-EW7WD46PYAU43WWZS4QZ/ke17ZwdGBToddI8pDm48kAXtGtTuS2U1SVcl-tYMBOAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8PaoYXhp6HxIwZIk7-Mi3Tsic-L2IOPH3Dwrhl-Ne3Z2YjE9w60pqfeJxDohDRZk1jXSVCSSfcEA7WmgMAGpjTehHAH51QaxKq4KdVMVBxpG/1nktc1kdgq2.jpg?format=1000w" width="175" title="colormaps" alt="DLC Utils" align="right" vspace = "50">
<p>Next, open the <strong>config.yaml</strong> file, which was created during  <strong>create_new_project</strong>. You can edit this file in any text editor.  Familiarize yourself with the meaning of the parameters (Box 1). You can edit various parameters, in particular you <strong>must add the list of <em>bodyparts</em> (or points of interest)</strong> that you want to track. You can also set the <em>colormap</em> here that is used for all downstream steps (can also be edited at anytime), like labeling GUIs, videos, etc. Here any <a class="reference external" href="https://matplotlib.org/tutorials/colors/colormaps.html">matplotlib colormaps</a> will do!</p>
<p>An easy way to programmatically edit the config file at any time is to use the function <strong>edit_config</strong>, which takes the full path of the config file to edit and a dictionary of key–value pairs to overwrite.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">edits</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;colormap&#39;</span><span class="p">:</span> <span class="s1">&#39;summer&#39;</span><span class="p">,</span>
         <span class="s1">&#39;individuals&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;mickey&#39;</span><span class="p">,</span> <span class="s1">&#39;minnie&#39;</span><span class="p">,</span> <span class="s1">&#39;bianca&#39;</span><span class="p">],</span>
         <span class="s1">&#39;skeleton&#39;</span><span class="p">:</span> <span class="p">[[</span><span class="s1">&#39;snout&#39;</span><span class="p">,</span> <span class="s1">&#39;tailbase&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;snout&#39;</span><span class="p">,</span> <span class="s1">&#39;rightear&#39;</span><span class="p">]]}</span>
<span class="n">deeplabcut</span><span class="o">.</span><span class="n">auxiliaryfunctions</span><span class="o">.</span><span class="n">edit_config</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">edits</span><span class="p">)</span>
</pre></div>
</div>
<p>Please DO NOT have spaces in the names of bodyparts, uniquebodyparts, individuals, etc.</p>
<p><strong>ATTENTIONt:</strong> You need to edit the config.yaml file to <strong>modify the following items</strong> which specify the animal ID, body parts, and any unique labels. Note, we also highly recommend that you use <strong>more bodypoints</strong> that you might be interested in for your experiment, i.e., labeling along the spine/tail for 8 bodypoints would be better than four. This will help the performance.</p>
<p>Modifying the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> is crucial:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">individuals</span><span class="p">:</span>
<span class="o">-</span> <span class="n">m1</span>
<span class="o">-</span> <span class="n">m2</span>
<span class="o">-</span> <span class="n">m3</span>

<span class="n">uniquebodyparts</span><span class="p">:</span>
<span class="o">-</span> <span class="n">topleftcornerofBox</span>
<span class="o">-</span> <span class="n">toprightcornerofBox</span>

<span class="n">multianimalbodyparts</span><span class="p">:</span>
<span class="o">-</span> <span class="n">snout</span>
<span class="o">-</span> <span class="n">leftear</span>
<span class="o">-</span> <span class="n">rightear</span>
<span class="o">-</span> <span class="n">tailbase</span>

<span class="n">identity</span><span class="p">:</span> <span class="kc">True</span><span class="o">/</span><span class="kc">False</span>
</pre></div>
</div>
<p><strong>Individuals:</strong> are names of “individuals” in the annotation dataset. These should/can be generic (e.g. mouse1, mouse2, etc.). These individuals are comprised of the same bodyparts defined by <code class="docutils literal notranslate"><span class="pre">multianimalbodyparts</span></code>. For annotation in the GUI and training, it is important that all individuals in each frame are labeled. Thus, keep in mind that you need to set individuals to the maximum number in your labeled-data set, .i.e., if there is (even just one frame) with 17 animals then the list should be <code class="docutils literal notranslate"><span class="pre">-</span> <span class="pre">indv1</span></code> to <code class="docutils literal notranslate"><span class="pre">-</span> <span class="pre">indv17</span></code>. Note, once trained if you have a video with more or less animals, that is fine - you can have more or less animals during video analysis!</p>
<p><strong>Identity:</strong> If you can tell the animals apart, i.e.,  one might have a collar, or a black marker on the tail of a mouse, then you should label these individuals consistently (i.e., always label the mouse with the black marker as “indv1”, etc). If you have this scenario, please set <code class="docutils literal notranslate"><span class="pre">identity:</span> <span class="pre">True</span></code> in your <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> file. If you have 4 black mice, and you truly cannot tell them apart, then leave this as <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p>
<p><strong>Multianimalbodyparts:</strong> are the bodyparts of each individual (in the above list).</p>
<p><strong>Uniquebodyparts:</strong> are points that you want to track, but that appear only once within each frame, i.e. they are “unique”. Typically these are things like unique objects, landmarks, tools, etc. They can also be animals, e.g. in the case where one German shepherd is attending to many sheep the sheep bodyparts would be multianimalbodyparts, the shepherd parts would be uniquebodyparts and the individuals would be the list of sheep (e.g. Polly, Molly, Dolly, …).</p>
</div>
<div class="section" id="select-frames-to-label">
<h3>Select Frames to Label:<a class="headerlink" href="#select-frames-to-label" title="Permalink to this headline">¶</a></h3>
<p><strong>CRITICAL:</strong> A good training dataset should consist of a sufficient number of frames that capture the breadth of the behavior. This ideally implies to select the frames from different (behavioral) sessions, different lighting and different animals, if those vary substantially (to train an invariant, robust feature detector). Thus for creating a robust network that you can reuse in the laboratory, a good training dataset should reflect the diversity of the behavior with respect to postures, luminance conditions, background conditions, animal identities, etc. of the data that will be analyzed. For the simple lab behaviors comprising mouse reaching, open-field behavior and fly behavior, 100−200 frames gave good results <a class="reference external" href="https://www.nature.com/articles/s41593-018-0209-y">Mathis et al, 2018</a>. However, depending on the required accuracy, the nature of behavior, the video quality (e.g. motion blur, bad lighting) and the context, more or less frames might be necessary to create a good network. Ultimately, in order to scale up the analysis to large collections of videos with perhaps unexpected conditions, one can also refine the data set in an adaptive way (see refinement below). <strong>For maDLC, be sure you have labeled frames with closely interacting animals!</strong></p>
<p>The function <code class="docutils literal notranslate"><span class="pre">extract_frames</span></code> extracts frames from all the videos in the project configuration file in order to create a training dataset. The extracted frames from all the videos are stored in a separate subdirectory named after the video file’s name under the ‘labeled-data’. This function also has various parameters that might be useful based on the user’s need.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_frames</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;automatic/manual&#39;</span><span class="p">,</span> <span class="n">algo</span><span class="o">=</span><span class="s1">&#39;uniform/kmeans&#39;</span><span class="p">,</span> <span class="n">userfeedback</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">crop</span><span class="o">=</span><span class="kc">True</span><span class="o">/</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>CRITICAL POINT:</strong> It is advisable to keep the frame size small, as large frames increase the training and
inference time, or you might not have a large enough GPU for this.
When running the function <code class="docutils literal notranslate"><span class="pre">extract_frames</span></code>, if the parameter crop=True, then you will be asked to draw a box within the GUI (and this is written to the config.yaml file).</p>
<p><code class="docutils literal notranslate"><span class="pre">userfeedback</span></code> allows the user to check which videos they wish to extract frames from. In this way, if you added more videos to the config.yaml file it does not, by default, extract frames (again) from every video. If you wish to disable this question, set <code class="docutils literal notranslate"><span class="pre">userfeedback</span> <span class="pre">=</span> <span class="pre">True</span></code>.</p>
<p>The provided function either selects frames from the videos in a randomly and temporally uniformly distributed
way (uniform), by clustering based on visual appearance (k-means), or by manual selection. Random
selection of frames works best for behaviors where the postures vary across the whole video. However, some behaviors
might be sparse, as in the case of reaching where the reach and pull are very fast and the mouse is not moving much
between trials (thus, we have the default set to True, as this is best for most use-cases we encounter). In such a case, the function that allows selecting frames based on k-means derived quantization would
be useful. If the user chooses to use k-means as a method to cluster the frames, then this function downsamples the
video and clusters the frames using k-means, where each frame is treated as a vector. Frames from different clusters
are then selected. This procedure makes sure that the frames look different. However, on large and long videos, this
code is slow due to computational complexity.</p>
<p><strong>CRITICAL POINT:</strong> It is advisable to extract frames from a period of the video that contains interesting
behaviors, and not extract the frames across the whole video. This can be achieved by using the start and stop
parameters in the config.yaml file. Also, the user can change the number of frames to extract from each video using
the numframes2extract in the config.yaml file.</p>
<p><strong>For maDLC, be sure you have labeled frames with closely interacting animals!</strong> Therefore, manually selecting some frames is a good idea if interactions are not highly frequent in the video.</p>
<p>However, picking frames is highly dependent on the data and the behavior being studied. Therefore, it is hard to
provide all purpose code that extracts frames to create a good training dataset for every behavior and animal. If the user feels specific frames are lacking, they can extract hand selected frames of interest using the interactive GUI
provided along with the toolbox. This can be launched by using:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_frames</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="s1">&#39;manual&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The user can use the <em>Load Video</em> button to load one of the videos in the project configuration file, use the scroll
bar to navigate across the video and <em>Grab a Frame</em> (or a range of frames, as of version 2.0.5) to extract the frame(s). The user can also look at the extracted frames and e.g. delete frames (from the directory) that are too similar before reloading the set and then manually annotating them.</p>
</div>
<div class="section" id="label-frames">
<h3>Label Frames:<a class="headerlink" href="#label-frames" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">label_frames</span><span class="p">(</span><span class="n">config_path</span><span class="p">)</span>
</pre></div>
</div>
<p>As of 2.2 there is a new multi-animal labeling GUI (as long as in your <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> says <code class="docutils literal notranslate"><span class="pre">multianimalproject:</span> <span class="pre">true</span></code> at the top, this will automatically launch).</p>
<p>The toolbox provides a function <strong>label_frames</strong> which helps the user to easily label all the extracted frames using
an interactive graphical user interface (GUI). The user should have already named the body parts to label (points of
interest) in the project’s configuration file by providing a list. The following command invokes the labeling toolbox.</p>
<p>The user needs to use the <em>Load Frames</em> button to select the directory which stores the extracted frames from one of
the videos. Subsequently, the user can use one of the radio buttons (top right) to select a body part to label. <strong>RIGHT</strong> click to add the label. Left click to drag the label, if needed. If you label a part accidentally, you can use the middle button on your mouse to delete (or hit the delete key while you hover over the point)! If you cannot see a body part in the frame, skip over the label! Please see the <code class="docutils literal notranslate"><span class="pre">HELP</span></code> button for more user instructions. This auto-advances once you labeled the first body part. You can also advance to the next frame by clicking on the RIGHT arrow on your keyboard (and go to a previous frame with LEFT arrow).
Each label will be plotted as a dot in a unique color.</p>
<p>The user is free to move around the body part and once satisfied with its position, can select another radio button
(in the top right) to switch to the respective body part (it otherwise auto-advances). The user can skip a body part if it is not visible. Once all the visible body parts are labeled, then the user can use ‘Next Frame’ to load the following frame. The user needs to save the labels after all the frames from one of the videos are labeled by clicking the save button at the bottom right. Saving the labels will create a labeled dataset for each video in a hierarchical data file format (HDF) in the
subdirectory corresponding to the particular video in <strong>labeled-data</strong>. You can save at any intermediate step (even without closing the GUI, just hit save) and you return to labeling a dataset by reloading it!</p>
<p><strong>CRITICAL POINT:</strong> It is advisable to <strong>consistently label similar spots</strong> (e.g., on a wrist that is very large, try
to label the same location). In general, invisible or occluded points should not be labeled by the user, unless you want to teach the network to “guess” - this is possible, but could affect accuracy. If you don’t want/or don’t see a bodypart, they can simply be skipped by not applying the label anywhere on the frame.</p>
<p>OPTIONAL: In the event of adding more labels to the existing labeled dataset, the user need to append the new
labels to the bodyparts in the config.yaml file. Thereafter, the user can call the function <strong>label_frames</strong>. A box will pop up and ask the user if they wish to display all parts, or only add in the new labels. Saving the labels after all the images are labelled will append the new labels to the existing labeled dataset.</p>
<p><strong>maDeepLabCut CRITICAL POINT:</strong> For multi-animal labeling, unless you can tell apart the animals, you do not need to worry about the “ID” of each animal. For example: if you have a white and black mouse label the white mouse as animal 1, and black as animal 2 across all frames. If two black mice, then the ID label 1 or 2 can switch between frames - no need for you to try to identify them (but always label consistently within a frame). If you have 2 black mice but one always has an optical fiber (for example), then DO label them consistently as animal1 and animal_fiber (for example). The point of multi-animal DLC is to train models that can first group the correct bodyparts to individuals, then associate those points in a given video to a specific individual, which then also uses temporal information to link across the video frames.</p>
<p>Note, we also highly recommend that you use more bodypoints that you might otherwise have (see the example below).</p>
<p><strong>Example Labeling with maDeepLabCut:</strong></p>
<ul class="simple">
<li><p>note you should within an animal be consistent, i.e., all bodyparts on mouse1 should be on mouse1, but across frames “mouse1” can be any of the black mice (as here it is nearly impossible to tell them apart visually). IF you can tell them apart, do label consistently!</p></li>
</ul>
<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1588028248844-43RXXUNLE1VKJDKGGVFO/ke17ZwdGBToddI8pDm48kAxoZwLd0g_s-irkR9O2vUhZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpxFjgZOWy5voI9x7QCcY8v6pdjAnJRY2VhSKj43SxhWXRPK8F08AQobuqKWFB6l9T0/labelingdemo.gif?format=750w" width="70%">
</p>
</div>
<div class="section" id="check-annotated-frames">
<h3>Check Annotated Frames:<a class="headerlink" href="#check-annotated-frames" title="Permalink to this headline">¶</a></h3>
<p>Checking if the labels were created and stored correctly is beneficial for training, since labeling
is one of the most critical parts for creating the training dataset. The DeepLabCut toolbox provides a function
‘check_labels’ to do so. It is used as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">check_labels</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">visualizeindividuals</span><span class="o">=</span><span class="kc">True</span><span class="o">/</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>maDeepLabCut:</strong> you can check and plot colors per individual or per body part, just set the flag <code class="docutils literal notranslate"><span class="pre">visualizeindividuals=True/False</span></code>. Note, you can run this twice in both states to see both images.</p>
<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1586203062876-D9ZL5Q7NZ464FUQN95NA/ke17ZwdGBToddI8pDm48kKmw982fUOZVIQXHUCR1F55Zw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpx7krGdD6VO1HGZR3BdeCbrijc_yIxzfnirMo-szZRSL5-VIQGAVcQr6HuuQP1evvE/img1068_individuals.png?format=750w" width="50%">
</p>
<p>For each video directory in labeled-data this function creates a subdirectory with <strong>labeled</strong> as a suffix. Those directories contain the frames plotted with the annotated body parts. The user can double check if the body parts are labeled correctly. If they are not correct, the user can reload the frames (i.e. <code class="docutils literal notranslate"><span class="pre">deeplabcut.label_frames</span></code>), move them around, and click save again.</p>
<p><strong>CROP+LABEL:</strong> When you are done checking the label quality and adjusting if needed, please then use this new function to crop frames /labels for more efficient training. PLEASE call this before you create a training dataset by running:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">cropimagesandlabels</span><span class="p">(</span><span class="n">path_config_file</span><span class="p">,</span> <span class="n">userfeedback</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="create-training-dataset">
<h3>Create Training Dataset:<a class="headerlink" href="#create-training-dataset" title="Permalink to this headline">¶</a></h3>
<p>Ideally for training DNNs, one uses large batch sizes. Thus, for mutli-animal training batch processing is preferred. This means that we’d like the images to be similarly sized. You can of course have differing size of images you label (but we suggest cropping out useless pixels!). So, we have a new function that can pre-process your data to be compatible with batch training. As noted above, please run this function before you <code class="docutils literal notranslate"><span class="pre">create_multianmialtraining_dataset</span></code>. This function assures that each crop is “small”, by default 400 x 400, which allows larger batchsizes and that there are multiple crops so that different parts of larger images are covered.</p>
<p>You <strong>should also first run</strong> <code class="docutils literal notranslate"><span class="pre">deeplabcut.cropimagesandlabels(config_path)</span></code> before creating a training set, as we use batch processing and many users have smaller GPUs that cannot accommodate larger images + larger batchsizes. This is also a type of data augmentation.</p>
<p>NOTE: you can edit the crop size. If your images are very large (2k, 4k pixels), consider increasing this size, but be aware unless you have a lagre GPU (24 GB or more), you will hit memory errors. <em>You can lower the batchsize, but this may affect performance.</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">cropimagesandlabels</span><span class="p">(</span><span class="n">path_config_file</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">400</span><span class="p">),</span> <span class="n">userfeedback</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>At this point you also select your neural network type. Please see Lauer et al. 2021 for options. For <strong>create_multianimaltraining_dataset</strong> we already changed this such that by default you will use imgaug, ADAM optimization, our new DLCRNet, and batch training. We suggest these defaults at this time. Then run:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_multianimaltraining_dataset</span><span class="p">(</span><span class="n">path_config_file</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The set of arguments in the function will shuffle the combined labeled dataset and split it to create train and test
sets. The subdirectory with suffix <code class="docutils literal notranslate"><span class="pre">iteration#</span></code> under the directory <strong>training-datasets</strong> stores the dataset and meta
information, where the <code class="docutils literal notranslate"><span class="pre">#</span></code> is the value of <code class="docutils literal notranslate"><span class="pre">iteration</span></code> variable stored in the project’s configuration file (this number
keeps track of how often the dataset was refined).</p></li>
<li><p>OPTIONAL: If the user wishes to benchmark the performance of the DeepLabCut, they can create multiple
training datasets by specifying an integer value to the <code class="docutils literal notranslate"><span class="pre">num_shuffles</span></code>; see the docstring for more details.</p></li>
<li><p>Each iteration of the creation of a training dataset will create several files, which is used by the feature detectors,
and a <code class="docutils literal notranslate"><span class="pre">.pickle</span></code> file that contains the meta information about the training dataset. This also creates two subdirectories
within <strong>dlc-models</strong> called <code class="docutils literal notranslate"><span class="pre">test</span></code> and <code class="docutils literal notranslate"><span class="pre">train</span></code>, and these each have a configuration file called pose_cfg.yaml.
Specifically, the user can edit the <strong>pose_cfg.yaml</strong> within the <strong>train</strong> subdirectory before starting the training. These
configuration files contain meta information with regard to the parameters of the feature detectors. Key parameters
are listed in Box 2.</p></li>
<li><p>At this step, the ImageNet pre-trained networks (i.e. ResNet-50) weights will be downloaded. If they do not download (you will see this downloading in the terminal, then you may not have permission to do so (something we have seen with some Windows users - see the <strong><a class="reference external" href="https://github.com/AlexEMG/DeepLabCut/wiki/Troubleshooting-Tips">WIKI troubleshooting for more help!</a></strong>).</p></li>
</ul>
<p><strong>OPTIONAL POINTS:</strong></p>
<p>With the data-driven skeleton selection introduced in 2.2rc1, DLC networks are trained by default
on complete skeletons (i.e., they learn all possible redundant connections), before being optimially pruned
at model evaluation. Although this procedure is by far superior to manually defining a graph,
we leave manually-defining a skeleton as an option for the advanced user:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">my_better_graph</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>  <span class="c1"># These are indices in the list of multianimalbodyparts</span>
<span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_multianimaltraining_dataset</span><span class="p">(</span><span class="n">path_config_file</span><span class="p">,</span> <span class="n">paf_graph</span><span class="o">=</span><span class="n">my_better_graph</span><span class="p">)</span>
</pre></div>
</div>
<p>Importantly, a user-defined graph is still required to cover all multianimalbodyparts at least once.</p>
<p><strong>DATA AUGMENTATION:</strong> At this stage you can also decide what type of augmentation to use. The default loaders work well for most all tasks (as shown on <a class="reference external" href="http://www.deeplabcut.org">www.deeplabcut.org</a>), but there are many options, more data augmentation, intermediate supervision, etc. Please look at the <a class="reference external" href="https://github.com/AlexEMG/DeepLabCut/blob/master/deeplabcut/pose_cfg.yaml"><strong>pose_cfg.yaml</strong></a> file for a full list of parameters <strong>you might want to change before running this step.</strong> There are several data loaders that can be used. For example, you can use the default loader (introduced and described in the Nature Protocols paper), <a class="reference external" href="https://github.com/tensorpack/tensorpack">TensorPack</a> for data augmentation (currently this is easiest on Linux only), or <a class="reference external" href="https://imgaug.readthedocs.io/en/latest/">imgaug</a>. We recommend <code class="docutils literal notranslate"><span class="pre">imgaug</span></code> (which is default now!). You can set this by passing:<code class="docutils literal notranslate"><span class="pre">deeplabcut.create_training_dataset(config_path,</span> <span class="pre">augmenter_type='imgaug')</span> </code></p>
<p>The differences of the loaders are as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">default</span></code>: our standard DLC 2.0 introduced in Nature Protocols variant (scaling, auto-crop augmentation) <em>will be renamed to <code class="docutils literal notranslate"><span class="pre">crop_scale</span></code> in a future release!</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">imgaug</span></code>: a lot of augmentation possibilities, efficient code for target map creation &amp; batch sizes &gt;1 supported. You can set the parameters such as the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> in the <code class="docutils literal notranslate"><span class="pre">pose_cfg.yaml</span></code> file for the model you are training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensorpack</span></code>: a lot of augmentation possibilities, multi CPU support for fast processing, target maps are created less efficiently than in imgaug, does not allow batch size&gt;1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">deterministic</span></code>: only useful for testing, freezes numpy seed; otherwise like default.</p></li>
</ul>
<p>Our recent <a class="reference external" href="https://www.cell.com/neuron/pdf/S0896-6273(20)30717-0.pdf">A Primer on Motion Capture with Deep Learning: Principles, Pitfalls, and Perspectives</a>, details the advantage of augmentation for a worked example (see Fig 7). TL;DR: use imgaug and use the symmetries of your data!</p>
<p>Alternatively, you can set the loader (as well as other training parameters) in the <strong>pose_cfg.yaml</strong> file of the model that you want to train. Note, to get details on the options, look at the default file: <a class="reference external" href="https://github.com/AlexEMG/DeepLabCut/blob/master/deeplabcut/pose_cfg.yaml"><strong>pose_cfg.yaml</strong></a>.</p>
</div>
<div class="section" id="train-the-network">
<h3>Train The Network:<a class="headerlink" href="#train-the-network" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">train_network</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">allow_growth</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>The set of arguments in the function starts training the network for the dataset created for one specific shuffle. Note that you can change the loader (imgaug/default/etc) as well as other training parameters in the <strong>pose_cfg.yaml</strong> file of the model that you want to train (before you start training).</p>
<p>Example parameters that one can call:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">train_network</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">trainingsetindex</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">gputouse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_snapshots_to_keep</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">autotune</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">displayiters</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">saveiters</span><span class="o">=</span><span class="mi">15000</span><span class="p">,</span> <span class="n">maxiters</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span> <span class="n">allow_growth</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>By default, the pretrained networks are not in the DeepLabCut toolbox (as they are around 100MB each), but they get downloaded before you train. However, if not previously downloaded from the TensorFlow model weights, it will be downloaded and stored in a subdirectory <em>pre-trained</em> under the subdirectory <em>models</em> in <em>Pose_Estimation_Tensorflow</em>.
At user specified iterations during training checkpoints are stored in the subdirectory <em>train</em> under the respective iteration directory.</p>
<p>If the user wishes to restart the training at a specific checkpoint they can specify the full path of the checkpoint to
the variable <code class="docutils literal notranslate"><span class="pre">init_weights</span></code> in the <strong>pose_cfg.yaml</strong> file under the <em>train</em> subdirectory (see Box 2).</p>
<p><strong>CRITICAL POINT:</strong> It is recommended to train the networks for thousands of iterations until the loss plateaus (typically around <strong>500,000</strong>) if you use batch size 1, and <strong>50-100K</strong> if you use batchsize 8 (the default).</p>
<p>If you use <strong>maDeepLabCut</strong> the recommended training iterations is <strong>20K-100K</strong> (it automatically stops at 200K!), as we use Adam and batchsize 8; if you have to reduce the batchsize for memory reasons then the number of iterations needs to be increased.</p>
<p>The variables <code class="docutils literal notranslate"><span class="pre">display_iters</span></code> and <code class="docutils literal notranslate"><span class="pre">save_iters</span></code> in the <strong>pose_cfg.yaml</strong> file allows the user to alter how often the loss is displayed and how often the weights are stored.</p>
<p><strong>maDeepLabCut CRITICAL POINT:</strong> For multi-animal projects we are using not only different and new output layers, but also new data augmentation, optimization, learning rates, and batch training defaults. Thus, please use a lower <code class="docutils literal notranslate"><span class="pre">save_iters</span></code> and <code class="docutils literal notranslate"><span class="pre">maxiters</span></code>. I.e. we suggest saving every 10K-15K iterations, and only training until 50K-100K iterations. We recommend you look closely at the loss to not overfit on your data. The bonus, training time is much less!!!</p>
<p><strong>Parameters:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>config : string
    Full path of the config.yaml file as a string.

shuffle: int, optional
    Integer value specifying the shuffle index to select for training. Default is set to 1

trainingsetindex: int, optional
    Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).

gputouse: int, optional. Natural number indicating the number of your GPU (see number in nvidia-smi). If you do not have a GPU, put None.
See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

max_snapshots_to_keep: int, or None. Sets how many snapshots are kept, i.e. states of the trained network. For every saving interation a snapshot is stored, however, only the last max_snapshots_to_keep many are kept! If you change this to None, then all are kept.
See: https://github.com/AlexEMG/DeepLabCut/issues/8#issuecomment-387404835

autotune: property of TensorFlow, somehow faster if &#39;false&#39; (as Eldar found out, see https://github.com/tensorflow/tensorflow/issues/13317). Default: False

displayiters: this variable is actually set in pose_config.yaml. However, you can overwrite it with this hack. Don&#39;t use this regularly, just if you are too lazy to dig out
the pose_config.yaml file for the corresponding project. If None, the value from there is used, otherwise it is overwritten! Default: None

saveiters: this variable is actually set in pose_config.yaml. However, you can overwrite it with this hack. Don&#39;t use this regularly, just if you are too lazy to dig out
the pose_config.yaml file for the corresponding project. If None, the value from there is used, otherwise it is overwritten! Default: None

maxiters: This sets how many iterations to train. This variable is set in pose_config.yaml. However, you can overwrite it with this. If None, the value from there is used, otherwise it is overwritten! Default: None
</pre></div>
</div>
</div>
<div class="section" id="evaluate-the-trained-network">
<h3>Evaluate the Trained Network:<a class="headerlink" href="#evaluate-the-trained-network" title="Permalink to this headline">¶</a></h3>
<p>Here, for traditional projects you will get a pixel distance metric and you should inspect the individual frames:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">evaluate_network</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">plotting</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>:movie_camera:<a class="reference external" href="https://www.youtube.com/watch?v=bgfnz1wtlpo">VIDEO TUTORIAL AVAILABLE!</a></p>
<p>It is important to evaluate the performance of the trained network. This performance is measured by computing
the mean average Euclidean error (MAE; which is proportional to the average root mean square error) between the
manual labels and the ones predicted by DeepLabCut. The MAE is saved as a comma separated file and displayed
for all pairs and only likely pairs (&gt;p-cutoff). This helps to exclude, for example, occluded body parts. One of the
strengths of DeepLabCut is that due to the probabilistic output of the scoremap, it can, if sufficiently trained, also
reliably report if a body part is visible in a given frame. (see discussions of finger tips in reaching and the Drosophila
legs during 3D behavior in [Mathis et al, 2018]). The evaluation results are computed by typing:</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">plotting</span></code> to true plots all the testing and training frames with the manual and predicted labels. The user
should visually check the labeled test (and training) images that are created in the ‘evaluation-results’ directory.
Ideally, DeepLabCut labeled unseen (test images) according to the user’s required accuracy, and the average train
and test errors are comparable (good generalization). What (numerically) comprises an acceptable MAE depends on
many factors (including the size of the tracked body parts, the labeling variability, etc.). Note that the test error can
also be larger than the training error due to human variability (in labeling, see Figure 2 in Mathis et al, Nature Neuroscience 2018).</p>
<p><strong>Optional parameters:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  Shuffles: list, optional -List of integers specifying the shuffle indices of the training dataset. The default is [1]

  plotting: bool, optional -Plots the predictions on the train and test images. The default is `False`; if provided it must be either `True` or `False`

  show_errors: bool, optional -Display train and test errors. The default is `True`

  comparisonbodyparts: list of bodyparts, Default is all -The average error will be computed for those body parts only (Has to be a subset of the body parts).

  gputouse: int, optional -Natural number indicating the number of your GPU (see number in nvidia-smi). If you do not have a GPU, put None. See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries
</pre></div>
</div>
<p>The plots can be customized by editing the <strong>config.yaml</strong> file (i.e., the colormap, scale, marker size (dotsize), and
transparency of labels (alphavalue) can be modified). By default each body part is plotted in a different color
(governed by the colormap) and the plot labels indicate their source. Note that by default the human labels are
plotted as plus (‘+’), DeepLabCut’s predictions either as ‘.’ (for confident predictions with likelihood &gt; p-cutoff) and
’x’ for (likelihood &lt;= <code class="docutils literal notranslate"><span class="pre">pcutoff</span></code>).</p>
<p>The evaluation results for each shuffle of the training dataset are stored in a unique subdirectory in a newly created
directory ‘evaluation-results’ in the project directory. The user can visually inspect if the distance between the labeled
and the predicted body parts are acceptable. In the event of benchmarking with different shuffles of same training
dataset, the user can provide multiple shuffle indices to evaluate the corresponding network. If the generalization is
not sufficient, the user might want to:</p>
<p>• check if the labels were imported correctly; i.e., invisible points are not labeled and the points of interest are
labeled accurately</p>
<p>• make sure that the loss has already converged</p>
<p>• consider labeling additional images and make another iteration of the training data set</p>
<p><strong>maDeepLabCut: (or on normal projects!)</strong></p>
<p>In multi-animal projects, model evaluation is crucial as this is when
the data-driven selection of the optimal skeleton is carried out. Skipping that step
causes video analysis to use the redundant skeleton by default, which is not only slow
but does not guarantee best performance.</p>
<p>You should also plot the scoremaps, locref layers, and PAFs to assess performance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_save_all_maps</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span> <span class="n">Indices</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
<p>you can drop “Indices” to run this on all training/testing images (this is very slow!)</p>
</div>
<div class="section" id="decision-break-point">
<h3>——————– DECISION / BREAK POINT ——————-<a class="headerlink" href="#decision-break-point" title="Permalink to this headline">¶</a></h3>
<div class="section" id="attention">
<h4>ATTENTION!<a class="headerlink" href="#attention" title="Permalink to this headline">¶</a></h4>
<p><strong>Pose estimation and tracking should be thought of as separate steps.</strong> If you do not have good pose estimation evaluation metrics at this point, stop, check original labels, add more data, etc –&gt; don’t move forward with this model. If you think you have a good model, plese test the “raw” pose estimation performance on a video to validate performance:</p>
<p>Please run:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scorername</span> <span class="o">=</span> <span class="n">deeplabcut</span><span class="o">.</span><span class="n">analyze_videos</span><span class="p">(</span><span class="n">config_path</span><span class="p">,[</span><span class="s1">&#39;/fullpath/project/videos/testVideo.mp4&#39;</span><span class="p">],</span> <span class="n">videotype</span><span class="o">=</span><span class="s1">&#39;.mp4&#39;</span><span class="p">)</span>
<span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_video_with_all_detections</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;/fullpath/project/videos/testVideo.mp4&#39;</span><span class="p">],</span> <span class="n">videotype</span><span class="o">=</span><span class="s1">&#39;.mp4&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Please note that you do <strong>not</strong> get the .h5/csv file you might be used to getting (this comes after tracking). You will get a <code class="docutils literal notranslate"><span class="pre">pickle</span></code> file that is used in <code class="docutils literal notranslate"><span class="pre">create_video_with_all_detections</span></code>. IF you have good clean out video, ending in <code class="docutils literal notranslate"><span class="pre">....full.mp4</span></code> (and the evaluation metrics look good, scoremaps look good, plotted evaluation images), then go forward!!!</p>
<p>If this does not look good, we recommend extracting and labeling more frames (even from more videos). Try to label close interactions of animals for best performance. Once you label more, you can create a new training set and train.</p>
<p>You can either:</p>
<ol class="simple">
<li><p>extract more frames from existing or new videos and label as when initially building the training data set, or</p></li>
<li><p>extract outlier frames based on the videos you just analyzed. To do this, you first need to analyze a video and convert to tracklets (Step 1 and 2 in the Analyze Video tab of the GUI), then you load the tracklet file into the refine tracklet GUI, and “Save” without making any modifications (or run <code class="docutils literal notranslate"><span class="pre">deeplabcut.convert_raw_tracks_to_h5(config_path,</span> <span class="pre">picklefile)</span></code>). That will create the files needed in the Extract Outlier Frames tab of the GUI.</p></li>
</ol>
</div>
</div>
<div class="section" id="animal-assembly-tracking-across-frames">
<h3>——————- ANIMAL ASSEMBLY &amp; TRACKING ACROSS FRAMES ——————-<a class="headerlink" href="#animal-assembly-tracking-across-frames" title="Permalink to this headline">¶</a></h3>
<p>After pose estimation, now you perform assembly and tracking. <em>NEW</em> in 2.2 is a novel data-driven way to set the optimal skeleton and assembly metrics, so this no longer requires user input. The metrics, in case you do want to edit them, can be found in the <code class="docutils literal notranslate"><span class="pre">inference_cfg.yaml</span></code> file.</p>
</div>
<div class="section" id="optimized-animal-assembly-video-analysis">
<h3>Optimized Animal Assembly + Video Analysis:<a class="headerlink" href="#optimized-animal-assembly-video-analysis" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Please note that <strong>novel videos DO NOT need to be added to the config.yaml file</strong>. You can simply have a folder elsewhere on your computer and pass the video folder (then it will analyze all videos of the specified type (i.e. <code class="docutils literal notranslate"><span class="pre">videotype='.mp4'</span></code>), or pass the path to the <strong>folder</strong> or exact video(s) you wish to analyze:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">analyze_videos</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;/fullpath/project/videos/&#39;</span><span class="p">],</span> <span class="n">videotype</span><span class="o">=</span><span class="s1">&#39;.mp4&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note: You do <strong>not</strong> get the .h5/csv file you might be used to getting in standard DLC (this comes next!).</p>
<p>Now that you have detections (which are saved as a pickle file, not h5, btw), we need to assemble and track the animals. This step has several tracker types (<code class="docutils literal notranslate"><span class="pre">track_method</span></code>), and we recommend testing which one works best on your data (but typically we find ellipse is best).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">convert_detections2tracklets</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;videofile_path&#39;</span><span class="p">],</span> <span class="n">videotype</span><span class="o">=</span><span class="s1">&#39;mp4&#39;</span><span class="p">,</span>
                                        <span class="n">shuffle</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">trainingsetindex</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">track_method</span><span class="o">=</span><span class="s1">&#39;box/ellipse/skeleton&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>You can validate the tracking parameters. Namely, you can iteratively change the parameters, run <code class="docutils literal notranslate"><span class="pre">convert_detections2tracklets</span></code> then load them in the GUI (<code class="docutils literal notranslate"><span class="pre">refine_tracklets</span></code>) if you want to look at the performance. If you want to edit these, you will need to open the <code class="docutils literal notranslate"><span class="pre">inference_cfg.yaml</span></code> file (or click button in GUI). The options are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Tracking:</span>
<span class="c1">#p/m pixels in width and height for increasing bounding boxes.</span>
<span class="n">boundingboxslack</span> <span class="p">:</span> <span class="mi">0</span>
<span class="c1"># Intersection over Union (IoU) threshold for linking two bounding boxes</span>
<span class="n">iou_threshold</span><span class="p">:</span> <span class="mf">.2</span>
<span class="c1"># maximum duration of a lost tracklet before it&#39;s considered a &quot;new animal&quot; (in frames)</span>
<span class="n">max_age</span><span class="p">:</span> <span class="mi">100</span>
<span class="c1"># minimum number of consecutive frames before a detection is tracked</span>
<span class="n">min_hits</span><span class="p">:</span> <span class="mi">3</span>
</pre></div>
</div>
<p><strong>IMPORTANT POINT</strong></p>
<p>If the network has been trained to learn the animals’ identities (i.e., you set <code class="docutils literal notranslate"><span class="pre">identity=True</span></code> in config.yaml before training)
this information can be leveraged both during: (i) animal assembly, where body parts
are grouped based on the animal they are predicted to belong to (affinity between pairs of keypoints
is no longer considered in that case); and (ii) animal tracking, where identity only can be
utilized in place of motion trackers to form tracklets.</p>
<p>To use this ID information, simply pass:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">convert_detections2tracklets</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">identity_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Animal assembly and tracking quality</strong> can be assessed via <code class="docutils literal notranslate"><span class="pre">deeplabcut.utils.make_labeled_video.create_video_from_pickled_tracks</span></code>. This function provides an additional diagnostic tool before moving on to refining tracklets.</p>
<p>**Next, tracklets are stitched to form complete tracks with:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">stitch_tracklets</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;videofile_path&#39;</span><span class="p">],</span> <span class="n">videotype</span><span class="o">=</span><span class="s1">&#39;mp4&#39;</span><span class="p">,</span>
                            <span class="n">shuffle</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">trainingsetindex</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">track_method</span><span class="o">=</span><span class="s1">&#39;box/ellipse/skeleton&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the base signature of the function is identical to <code class="docutils literal notranslate"><span class="pre">analyze_videos</span></code> and <code class="docutils literal notranslate"><span class="pre">convert_detections2tracklets</span></code>.
If the number of tracks to reconstruct is different from the number of individuals
originally defined in the config.yaml, <code class="docutils literal notranslate"><span class="pre">n_tracks</span></code> (i.e., the number of animals you have in your video)
can be directly specified as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">stitch_tracklets</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">n_tracks</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
<p>In such cases, file columns will default to dummy animal names (ind1, ind2, …, up to indn).</p>
</div>
<div class="section" id="refine-tracklets">
<h3>Refine Tracklets:<a class="headerlink" href="#refine-tracklets" title="Permalink to this headline">¶</a></h3>
<p>You can also optionally <strong>refine the tracklets</strong>. You can fix both “major” ID swaps, i.e. perhaps when animals cross, and you can micro-refine the individual body points. You will load the <code class="docutils literal notranslate"><span class="pre">...trackertype.pickle</span></code> file that was created above, and then you can launch a GUI to interactively refine the data. This also has several options, so please check out the docstring. Upon saving the refined tracks you get an <code class="docutils literal notranslate"><span class="pre">.h5</span></code> file (akin to what you might be used to from standard DLC. You can also load (1) filter this to take care of small jitters, and (2) load this <code class="docutils literal notranslate"><span class="pre">.h5</span></code> this to refine (again) in case you find another issue, etc!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">refine_tracklets</span><span class="p">(</span><span class="n">path_config_file</span><span class="p">,</span> <span class="n">pickle_or_h5_file</span><span class="p">,</span> <span class="n">videofile_path</span><span class="p">,</span> <span class="n">max_gap</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">min_swap_len</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_tracklet_len</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">trail_len</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
<p>If you use the GUI (or otherwise), here are some settings to consider:</p>
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1619628014395-BQ09VLLTKCLQQGRB5T9A/ke17ZwdGBToddI8pDm48kLMj_XrWI9gi4tVeBdgcB8p7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0lt53wR20brczws2A6XSGt3kSTbW7uM0ncVKHWPvgHR4kN5Ka1TcK96ljy4ji9jPkQ/TrackletGUI.png?format=1000w" width="950" title="maDLCtrack" alt="maDLC" align="center" vspace = "50">
<p>*note, setting <code class="docutils literal notranslate"><span class="pre">max_gap=0</span></code> can be used to fill in all frames across the video; otherwise, 1-n is the # of frames you want to fill in, i.e. maybe you want to fill in short gaps of 5 frames, but 15 frames indicates another issue, etc. You can test this in the GUI very easy by editing the value and then re-launch pop-up GUI.</p>
<p>If you fill in gaps, they will be associated to an ultra low probability, 0.01, so you are aware this is not the networks best estimate, this is the human-override! Thus, if you create a video, you need to set your pcutoff to 0 if you want to see these filled in frames.</p>
<p><a class="reference external" href="functionDetails.md#madeeplabcut-critical-point---assemble--refine-tracklets">Read more here!</a></p>
<p>Short demo:</p>
 <p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1588690928000-90ZMRIM8SN6QE20ZOMNX/ke17ZwdGBToddI8pDm48kJ1oJoOIxBAgRD2ClXVCmKFZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpxBw7VlGKDQO2xTcc51Yv6DahHgScLwHgvMZoEtbzk_9vMJY_JknNFgVzVQ2g0FD_s/refineDEMO.gif?format=750w" width="70%">
</p>
</div>
<div class="section" id="once-you-have-analyzed-video-data-and-refined-your-madeeplabcut-tracklets">
<h3>Once you have analyzed video data (and refined your maDeepLabCut tracklets):<a class="headerlink" href="#once-you-have-analyzed-video-data-and-refined-your-madeeplabcut-tracklets" title="Permalink to this headline">¶</a></h3>
<p>Firstly, Here are some tips for scaling up your video analysis, including looping over many folders for batch processing: <a class="reference external" href="https://github.com/AlexEMG/DeepLabCut/wiki/Batch-Processing-your-Analysis">https://github.com/AlexEMG/DeepLabCut/wiki/Batch-Processing-your-Analysis</a></p>
<p>You can also filter the predicted bodyparts by:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">filterpredictions</span><span class="p">(</span><span class="n">config_path</span><span class="p">,[</span><span class="s1">&#39;/fullpath/project/videos/reachingvideo1.avi&#39;</span><span class="p">],</span> <span class="n">track_method</span><span class="o">=</span><span class="s1">&#39;box/ellipse/skeleton&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note, this creates a file with the ending filtered.h5 that you can use for further analysis. This filtering step has many parameters, so please see the full docstring by typing: <code class="docutils literal notranslate"><span class="pre">deeplabcut.filterpredictions?</span></code></p>
</div>
<div class="section" id="plotting-results">
<h3>Plotting Results:<a class="headerlink" href="#plotting-results" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><strong>NOTE :bulb::mega::</strong> Before you create a video, you should set what threshold to use for plotting. This is set in the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> file as <code class="docutils literal notranslate"><span class="pre">pcutoff</span></code> - if you have a well trained network, this should be high, i.e. set it to <code class="docutils literal notranslate"><span class="pre">0.8</span></code> or higher! IF YOU FILLED IN GAPS, you need to set this to 0 to “see” the filled in parts.</p></li>
<li><p>You can also determine a good <code class="docutils literal notranslate"><span class="pre">pcutoff</span></code> value by looking at the likelihood plot created during <code class="docutils literal notranslate"><span class="pre">plot_trajectories</span></code>:</p></li>
</ul>
<p>Plot the outputs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">plot_trajectories</span><span class="p">(</span><span class="n">config_path</span><span class="p">,[</span><span class="s1">&#39;/fullpath/project/videos/reachingvideo1.avi&#39;</span><span class="p">],</span><span class="n">filtered</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">track_method</span><span class="o">=</span><span class="s1">&#39;box/ellipse/skeleton&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Create videos:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>deeplabcut.create_labeled_video(config_path, [`/analysis/project/videos/reachingvideo1.avi&#39;,&#39;/fullpath/project/videos/reachingvideo2.avi&#39;],filtered = True, track_method=&#39;box/ellipse/skeleton&#39;)
</pre></div>
</div>
<p>(more details <a class="reference external" href="functionDetails.md#i-video-analysis-and-plotting-results">here</a>)</p>
</div>
<div class="section" id="help">
<h3>HELP:<a class="headerlink" href="#help" title="Permalink to this headline">¶</a></h3>
<p>In ipython/Jupyter notebook:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>deeplabcut.nameofthefunction?
</pre></div>
</div>
<p>In python or pythonw:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">help</span><span class="p">(</span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">nameofthefunction</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="tips-for-daily-use">
<h3>Tips for “daily” use:<a class="headerlink" href="#tips-for-daily-use" title="Permalink to this headline">¶</a></h3>
<p align="center">
<img src= https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5ccc5abe0d9297405a428522/1556896461304/howtouseDLC-01.png?format=1000w width="80%">
 </p>
<p>You can always exit an conda environment and easily jump back into a project by simply:</p>
<p>Linux/MacOS formatting example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">source</span> <span class="n">activate</span> <span class="n">yourdeeplabcutEnvName</span>
<span class="n">ipython</span> <span class="ow">or</span> <span class="n">pythonw</span>
<span class="kn">import</span> <span class="nn">deeplabcut</span>
<span class="n">config_path</span> <span class="o">=</span><span class="s1">&#39;/home/yourprojectfolder/config.yaml&#39;</span>
</pre></div>
</div>
<p>Windows formatting example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">activate</span> <span class="n">yourdeeplabcutEnvName</span>
<span class="n">ipython</span>
<span class="kn">import</span> <span class="nn">deeplabcut</span>
<span class="n">config_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;C:\home\yourprojectfolder\config.yaml&#39;</span>
</pre></div>
</div>
<p>Now, you can run any of the functions described in this documentation.</p>
</div>
</div>
</div>
<div class="section" id="getting-help-with-madlc">
<h1>Getting help with maDLC:<a class="headerlink" href="#getting-help-with-madlc" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>If you have a detailed question about how to use the code, or you hit errors that are not “bugs” but you want code assistance, please post on the <a class="reference external" href="https://forum.image.sc/tags/deeplabcut"><img alt="Image.sc forum" src="https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;url=https%3A%2F%2Fforum.image.sc%2Ftags%2Fdeeplabcut.json&amp;query=%24.topic_list.tags.0.topic_count&amp;colorB=brightgreen&amp;&amp;suffix=%20topics&amp;logo=data:image/png%3Bbase64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC" /></a></p></li>
<li><p>If you have a quick, short question that fits a “chat” format:
<a class="reference external" href="https://gitter.im/DeepLabCut/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge"><img alt="Gitter" src="https://badges.gitter.im/DeepLabCut/community.svg" /></a></p></li>
<li><p>If you want to share some results, or see others:
<a class="reference external" href="https://twitter.com/DeepLabCut"><img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/DeepLabCut.svg?label=DeepLabCut&amp;style=social" /></a></p></li>
<li><p>If you have a code bug report, please create an issue and show the minimal code to reproduce the error: <a class="reference external" href="https://github.com/DeepLabCut/DeepLabCut/issues">https://github.com/DeepLabCut/DeepLabCut/issues</a></p></li>
<li><p>if you are looking for resources to increase your understanding of the software and general guidelines, we have an open source, free course: <a class="reference external" href="http://DLCcourse.deeplabcut.org">http://DLCcourse.deeplabcut.org</a>.</p></li>
</ul>
<p><strong>Please note:</strong> what we cannot do is provided support or help designing your experiments and data analysis. The number of requests for this is too great to sustain in our inbox. We are happy to answer such questions in the forum as a community, in a scalable way. We hope and believe we have given enough tools and resources to get started and to accelerate your research program, and this is backed by the &gt;700 citations using DLC, 2 clinical trials by others, and countless applications. Thus, we believe this code works, is accessible, and with limited programming knowledge can be used. Please read our <a class="reference external" href="https://github.com/DeepLabCut/DeepLabCut/blob/master/docs/MISSION_AND_VALUES.md">Missions &amp; Values statement</a> to learn more about what we DO hope to provide you.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="standardDeepLabCut_UserGuide.html" title="previous page">DeepLabCut User Guide (for single animal projects)</a>
    <a class='right-next' id="next-link" href="PROJECT_GUI.html" title="next page">Interactive Project Manager GUI</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The DeepLabCut Team<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            <div>Powered by <a href="https://jupyterbook.org/">Jupyter Book</a>.</div>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>